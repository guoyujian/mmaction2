{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计某一种动作在视频指定时间段中出现的次数\n",
    "说明：\n",
    "1. 某一动作是指：get/put down/screw/pump其中之一，标签0-3\n",
    "2. 指定时间段是由起始时间点和结束时间点确定。\n",
    "3. 可能是视频+指定时间，也有可能是帧文件夹+帧索引\n",
    "\n",
    "步骤（如果传入的是帧文件夹+ 帧索引）：\n",
    "1. 传参：帧文件夹路径frames_path，开始帧索引start_frame_index，结束帧索引end_frame_index，滑动窗口的大小window_size，滑动窗口的step\n",
    "2. 读入帧文件夹\n",
    "3. 取\\[start，end\\]，start = start_frame_index, end = start+window_size > end_frame_index ? end_frame_index : start_frame_index+window_size 生成一个关节点/骨架pkl文件，\n",
    "4. 将pkl送入动作分类模型进行推理，得到一个动作分类，添加到结果列表result_list中\n",
    "5. 取\\[start, end\\], start = start + step, end = end + step > end_frame_index ? end_frame_index: end + step，生成一个关节点/骨架pkl文件， 重复第四步\n",
    "6. 当start >= end_frame_index 时，停止循环\n",
    "7. 统计result_list动作出现的次数。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 优化思路\n",
    "1. 根据开始帧-结束帧索引，直接生成完整的骨骼点pkl文件\n",
    "2. 再滑动窗口，读取pkl文件，提取窗口内的骨骼点数据，重新生成子pkl文件，\n",
    "3. 将子pkl文件做推理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const\n",
    "# 根据关节点进行动作分类的配置文件和模型文件 \n",
    "CONFIG_FILE = 'slowonly_r50_u48_240e_ntu60_xsub_keypoint_4labels.py'\n",
    "# configfile = '/mmaction2/filling_exps/TimeSformer/timesformer.py'\n",
    "CHECKPOINT_FILE = '../checkpoints/ntu60_keypoints_4labels.pth'\n",
    "\n",
    "#pkl文件保存的路径\n",
    "PKL_DIR = './pkl_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "import os\n",
    "import os.path as osp\n",
    "from demo.demo_skeleton import  pose_inference, parse_args\n",
    "import argparse\n",
    "from mmcv import DictAction\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mmcv\n",
    "import torch\n",
    "from mmaction.apis import inference_recognizer, init_recognizer\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "\n",
    "try:\n",
    "    from mmdet.apis import inference_detector, init_detector\n",
    "except (ImportError, ModuleNotFoundError):\n",
    "    raise ImportError('Failed to import `inference_detector` and '\n",
    "                      '`init_detector` form `mmdet.apis`. These apis are '\n",
    "                      'required in this demo! ')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(cfg_options={}, checkpoint='https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint/slowonly_r50_u48_240e_ntu120_xsub_keypoint-6736b03f.pth', config='slowonly_r50_u48_240e_ntu60_xsub_keypoint_4labels.py', det_checkpoint='http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth', det_config='../demo/faster_rcnn_r50_fpn_2x_coco_modify.py', det_score_thr=0.9, device='cuda:0', label_map='../tools/data/skeleton/label_map_ntu120.txt', out_filename='../demo/demo_ske.mp4', pose_checkpoint='https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth', pose_config='../demo/hrnet_w32_coco_256x192.py', short_side=480, video='../demo/demo.mp4')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv = ['../demo/demo_skeleton.py', '../demo/demo.mp4', '../demo/demo_ske.mp4']\n",
    "# 暂时先用demo_ske的配置，后面改为自己的\n",
    "args = parse_args()\n",
    "# args.checkpoint = ''\n",
    "# args.config = '../configs/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint.py'\n",
    "args.config ='slowonly_r50_u48_240e_ntu60_xsub_keypoint_4labels.py'\n",
    "args.det_config = '../demo/faster_rcnn_r50_fpn_2x_coco_modify.py'\n",
    "args.pose_config='../demo/hrnet_w32_coco_256x192.py'\n",
    "args.label_map='../tools/data/skeleton/label_map_ntu120.txt'\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(folder_path, start_frame_index, end_frame_index):\n",
    "    '''\n",
    "    传入的路径是否存在，是否是文件夹，是否包含帧图\n",
    "    '''\n",
    "    if not osp.exists(folder_path) or not osp.isdir(folder_path):\n",
    "        return False\n",
    "    files = os.listdir(folder_path)\n",
    "    if 'img_00000.jpg' not in files:\n",
    "        print('img_00000.jpg 不存在')\n",
    "        return False\n",
    "    index_str = \"%05d\" % start_frame_index\n",
    "    if f'img_{index_str}.jpg' not in files:\n",
    "        print(f'img_{index_str}.jpg 不存在')\n",
    "        return False\n",
    "    index_str = \"%05d\" % end_frame_index\n",
    "    if f'img_{index_str}.jpg' not in files:\n",
    "        print(f'img_{index_str}.jpg 不存在')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def check_frame_index(start_frame_index, end_frame_index):\n",
    "    '''\n",
    "    检查index参数是否合法\n",
    "    '''\n",
    "    if start_frame_index < 0 or end_frame_index < 0 or start_frame_index >= end_frame_index:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(check_folder('/home/fate/openmmlab/mmaction2/data/13621115_5_0', 5873, 7539))\n",
    "# print(check_frame_index(5873, 7539))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_extraction(folder_path, start, end):\n",
    "    '''\n",
    "    读取帧文件夹中从start-end的帧图，返回帧图的路径列表和帧数组\n",
    "    '''\n",
    "    frame_paths = []\n",
    "    frames = []\n",
    "    new_h, new_w = None, None\n",
    "    for i in range(start, end):\n",
    "        index = \"%05d\" % i\n",
    "        filename = f'img_{index}.jpg'\n",
    "        frame_path = osp.join(folder_path, filename)\n",
    "        frame = cv2.imread(frame_path)\n",
    "\n",
    "        if new_h is None:\n",
    "            h, w, _ = frame.shape\n",
    "            new_w, new_h = mmcv.rescale_size((w, h), (480, np.Inf))\n",
    "\n",
    "        frame = mmcv.imresize(frame, (new_w, new_h))\n",
    "\n",
    "        frames.append(frame)\n",
    "        frame_paths.append(osp.join(folder_path, filename))\n",
    "\n",
    "    return frame_paths, frames\n",
    "\n",
    "# frame_paths, frames = frame_extraction('/home/fate/openmmlab/mmaction2/data/13621115_5_0', 5873, 7539)\n",
    "# len(frame_paths), len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def detection_inference(args, frame_paths, batch_size = 1):\n",
    "    \"\"\"Detect human boxes given frame paths.\n",
    "    修改的 demo_skeleton.detection_inference\n",
    "    Args:\n",
    "        args (argparse.Namespace): The arguments.\n",
    "        frame_paths (list[str]): The paths of frames to do detection inference.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: The human detection results.\n",
    "    \"\"\"\n",
    "    model = init_detector(args.det_config, args.det_checkpoint, args.device)\n",
    "    assert model.CLASSES[0] == 'person', ('We require you to use a detector '\n",
    "                                          'trained on COCO')\n",
    "    results = []\n",
    "    print('Performing Human Detection for each frame')\n",
    "    prog_bar = mmcv.ProgressBar(len(frame_paths))\n",
    "    if batch_size == 1:\n",
    "        for frame_path in frame_paths:\n",
    "            result = inference_detector(model, frame_path)\n",
    "            # We only keep human detections with score larger than det_score_thr\n",
    "            result = result[0][result[0][:, 4] >= args.det_score_thr]\n",
    "            results.append(result)\n",
    "            prog_bar.update()\n",
    "    elif batch_size > 1:\n",
    "        start = 0 \n",
    "        while start < len(frame_paths):\n",
    "            end = start + batch_size if start + batch_size < len(frame_paths) else len(frame_paths)\n",
    "            slice_paths = frame_paths[start : end]\n",
    "            results_part = inference_detector(model, slice_paths)\n",
    "            for result in results_part:\n",
    "                result = result[0][result[0][:, 4] >= args.det_score_thr]\n",
    "                results.append(result)\n",
    "            update_num = end - start\n",
    "            start += batch_size\n",
    "            prog_bar.update(update_num)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 3/3, 25.0 task/s, elapsed: 0s, ETA:     0sload checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 3/3, 26.7 task/s, elapsed: 0s, ETA:     0s"
     ]
    }
   ],
   "source": [
    "import os \n",
    "pth = '/home/fate/openmmlab/mmaction2/data/tmp'\n",
    "filenames = os.listdir(pth)\n",
    "filenames = [osp.join(pth, filename) for filename in filenames]\n",
    "r1 = detection_inference(args, filenames, batch_size = 1)\n",
    "r2 = detection_inference(args, filenames, batch_size = 2)\n",
    "with open('r1.txt', 'w') as f:\n",
    "    f.write(str(r1))\n",
    "with open('r2.txt', 'w') as f:\n",
    "    f.write(str(r2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_skeleton_by_frames_folder_with_start_and_end(folder_path, start, end, pkl_dir):\n",
    "    '''\n",
    "    生成骨架pkl文件，保存到pkl_dir中\n",
    "    返回保存的文件名，文件名命名格式为：{start}.pkl命名\n",
    "    '''\n",
    "    if not osp.exists(pkl_dir):\n",
    "        os.mkdir(pkl_dir)\n",
    "    frame_paths, original_frames = frame_extraction(folder_path, start, end)\n",
    "    num_frame = len(frame_paths)\n",
    "    h, w, _ = original_frames[0].shape\n",
    "\n",
    "    # Get clip_len, frame_interval and calculate center index of each clip\n",
    "    config = mmcv.Config.fromfile(args.config)\n",
    "    config.merge_from_dict(args.cfg_options)\n",
    "    for component in config.data.test.pipeline:\n",
    "        if component['type'] == 'PoseNormalize':\n",
    "            component['mean'] = (w // 2, h // 2, .5)\n",
    "            component['max_value'] = (w, h, 1.)\n",
    "\n",
    "    # Get Human detection results\n",
    "    det_results = detection_inference(args, frame_paths, batch_size= 32)\n",
    "    torch.cuda.empty_cache()\n",
    "    \n",
    "    pose_results = pose_inference(args, frame_paths, det_results)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    fake_anno = dict(\n",
    "        frame_dir='',\n",
    "        label=-1,\n",
    "        img_shape=(h, w),\n",
    "        original_shape=(h, w),\n",
    "        start_index=0,\n",
    "        modality='Pose',\n",
    "        total_frames=num_frame)\n",
    "    num_person = max([len(x) for x in pose_results])\n",
    "\n",
    "    num_keypoint = 17\n",
    "    keypoint = np.zeros((num_person, num_frame, num_keypoint, 2),\n",
    "                        dtype=np.float16)\n",
    "    keypoint_score = np.zeros((num_person, num_frame, num_keypoint),\n",
    "                              dtype=np.float16)\n",
    "    for i, poses in enumerate(pose_results):\n",
    "        for j, pose in enumerate(poses):\n",
    "            pose = pose['keypoints']\n",
    "            keypoint[j, i] = pose[:, :2]\n",
    "            keypoint_score[j, i] = pose[:, 2]\n",
    "    fake_anno['keypoint'] = keypoint\n",
    "    fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "    out_anno_filename = osp.join(pkl_dir, f'{start}_{end}.pkl')\n",
    "    print(f'\\nsaving anno file: {out_anno_filename}')\n",
    "\n",
    "    with open(out_anno_filename, 'wb') as f:\n",
    "        pickle.dump(fake_anno, f)\n",
    "    return out_anno_filename\n",
    "\n",
    "# make_out_anno_filename = make_skeleton_by_frames_folder_with_start_and_end('/home/fate/openmmlab/data/13621115_5_0', 5873, 5874, '/home/fate/openmmlab/data/pkls')\n",
    "\n",
    "# make_out_anno_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_the_number_by_pred_labels_list(y_preds:list, label:int):\n",
    "    '''\n",
    "    给出模型预测的动作label序列列表，和需要计算的统计的动作label，计算出现的次数\n",
    "    计算次数的规则：连续出现的相同标签算做一次，各动作累计，\n",
    "    例如动作标签列表如[0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2]，将连续的相同标签合并之后如\n",
    "    [0,1,0,2,0,1,0,1,0,2]\n",
    "    则label= 0时，返回5，label=1时返回3，label=2时返回2\n",
    "    '''\n",
    "    if len(y_preds) == 0:\n",
    "        return -1\n",
    "    last = y_preds[0]\n",
    "    # seq = [y_preds[0]]\n",
    "    count_map = {y_preds[0] : 1}\n",
    "    for y_pred in y_preds:\n",
    "        if last != y_pred:\n",
    "            if y_pred in count_map:\n",
    "                count_map[y_pred] += 1\n",
    "            else:\n",
    "                count_map[y_pred] = 1\n",
    "        last = y_pred\n",
    "            \n",
    "    print(f'count_map:{count_map}')\n",
    "    return count_map[label]\n",
    "\n",
    "# l = [0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2]\n",
    "# count_the_number_by_pred_labels_list(l, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_the_number_of_times_a_certain_type_of_action_appears_in_the_frames_folder(\n",
    "    folder_path: str,\n",
    "    start_frame_index: int,\n",
    "    end_frame_index: int,\n",
    "    window_size: int,\n",
    "    step: int,\n",
    "    action_label: int\n",
    "):\n",
    "    '''\n",
    "    folder_path: path of a folder which contains frames of a video,\n",
    "    start_frame_index,\n",
    "    end_frame_index,\n",
    "    window_size: size of slide window\n",
    "    step: slide step, 小于等于0，则step = window_size\n",
    "    action_label: 4 actions type :['get','put down','screw', 'pump']\n",
    "    '''\n",
    "    # 0. check params\n",
    "    if not check_folder(folder_path, start_frame_index, end_frame_index) or not check_frame_index(start_frame_index, end_frame_index):\n",
    "        print(\"请检查参数\")\n",
    "        return\n",
    "    if step <= 0:\n",
    "        step = window_size\n",
    "    start = start_frame_index\n",
    "    y_preds = []\n",
    "    print(f'start: {start_frame_index}, end_frame_index: {end_frame_index}; window_size: {window_size}; step: {step}')\n",
    "    pkl_file_fullpath = make_skeleton_by_frames_folder_with_start_and_end(folder_path, start_frame_index, end_frame_index, PKL_DIR)\n",
    "    with open(pkl_file_fullpath, \"rb\")as f:\n",
    "        b = pickle.load(f) # read\n",
    "    model = init_recognizer(CONFIG_FILE, CHECKPOINT_FILE, device='cuda:0')\n",
    "    pbar = tqdm(total = end_frame_index - start_frame_index)\n",
    "    while start <= end_frame_index:\n",
    "        end = end_frame_index if start + window_size > end_frame_index else start + window_size\n",
    "        # print(f'start:{start}; end:{end}')\n",
    "        # pkl_file_fullpath = make_skeleton_by_frames_folder_with_start_and_end(folder_path, start, end, PKL_DIR)\n",
    "        # 读取完整的pkl文件\n",
    "        tmp = b.copy()\n",
    "        tmp['keypoint'] = tmp['keypoint'][:, (start - start_frame_index): (end - start_frame_index), :, :]\n",
    "        tmp['keypoint_score'] = tmp['keypoint_score'][:, (start - start_frame_index): (end - start_frame_index), :]\n",
    "        tmp['total_frames'] = end - start\n",
    "        \n",
    "        results = inference_recognizer(model, tmp)\n",
    "        y_pred = results[0][0]\n",
    "        y_preds.append(y_pred)\n",
    "            \n",
    "        start += step\n",
    "        pbar.update(step)\n",
    "    pbar.close()\n",
    "    # print(y_preds)\n",
    "    return count_the_number_by_pred_labels_list(y_preds, action_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 5873, end_frame_index: 7539; window_size: 60; step: 30\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1666/1666, 28.1 task/s, elapsed: 59s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>] 1666/1666, 34.6 task/s, elapsed: 48s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/5873_7539.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1680it [00:30, 54.45it/s]                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_map:{0: 12, 1: 3, 3: 3, 2: 9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_the_number_of_times_a_certain_type_of_action_appears_in_the_frames_folder(\n",
    "    '/home/fate/openmmlab/mmaction2/data/13621115_5_0',\n",
    "    5873,\n",
    "    7539,\n",
    "    30*2,\n",
    "    30,\n",
    "    1\n",
    ")  ## 3min16s ## 2m5s"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. 解决batch不同，推理结果不同的问题\n",
    "2. 多找几个视频试试，\n",
    "3. 其他的推理的batch也改大\n",
    "4. 22机器数据备份"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be40d7e6a4cd261be2e5a1fb6fd78b6abc421e8295ca616a082915926260a7a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
