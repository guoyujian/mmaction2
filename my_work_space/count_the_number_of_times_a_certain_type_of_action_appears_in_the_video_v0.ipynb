{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 统计某一种动作在视频指定时间段中出现的次数\n",
    "说明：\n",
    "1. 某一动作是指：get/put down/screw/pump其中之一，标签0-3\n",
    "2. 指定时间段是由起始时间点和结束时间点确定。\n",
    "3. 可能是视频+指定时间，也有可能是帧文件夹+帧索引\n",
    "\n",
    "步骤（如果传入的是帧文件夹+ 帧索引）：\n",
    "1. 传参：帧文件夹路径frames_path，开始帧索引start_frame_index，结束帧索引end_frame_index，滑动窗口的大小window_size，滑动窗口的step\n",
    "2. 读入帧文件夹\n",
    "3. 取\\[start，end\\]，start = start_frame_index, end = start+window_size > end_frame_index ? end_frame_index : start_frame_index+window_size 生成一个关节点/骨架pkl文件，\n",
    "4. 将pkl送入动作分类模型进行推理，得到一个动作分类，添加到结果列表result_list中\n",
    "5. 取\\[start, end\\], start = start + step, end = end + step > end_frame_index ? end_frame_index: end + step，生成一个关节点/骨架pkl文件， 重复第四步\n",
    "6. 当start >= end_frame_index 时，停止循环\n",
    "7. 统计result_list动作出现的次数。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# const\n",
    "# 根据关节点进行动作分类的配置文件和模型文件 \n",
    "CONFIG_FILE = 'slowonly_r50_u48_240e_ntu60_xsub_keypoint_4labels.py'\n",
    "# configfile = '/mmaction2/filling_exps/TimeSformer/timesformer.py'\n",
    "CHECKPOINT_FILE = '../checkpoints/ntu60_keypoints_4labels.pth'\n",
    "\n",
    "#pkl文件保存的路径\n",
    "PKL_DIR = './pkl_dir'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fate/.conda/envs/py38/lib/python3.8/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/fate/.conda/envs/py38/lib/python3.8/site-packages/mmcv/__init__.py:20: UserWarning: On January 1, 2023, MMCV will release v2.0.0, in which it will remove components related to the training process and add a data transformation module. In addition, it will rename the package names mmcv to mmcv-lite and mmcv-full to mmcv. See https://github.com/open-mmlab/mmcv/blob/master/docs/en/compatibility.md for more details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# import\n",
    "import os\n",
    "import os.path as osp\n",
    "from demo.demo_skeleton import detection_inference, pose_inference, parse_args\n",
    "import argparse\n",
    "from mmcv import DictAction\n",
    "import sys\n",
    "import cv2\n",
    "import numpy as np\n",
    "import mmcv\n",
    "import torch\n",
    "from mmaction.apis import inference_recognizer, init_recognizer\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Namespace(cfg_options={}, checkpoint='https://download.openmmlab.com/mmaction/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint/slowonly_r50_u48_240e_ntu120_xsub_keypoint-6736b03f.pth', config='../configs/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint.py', det_checkpoint='http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth', det_config='../demo/faster_rcnn_r50_fpn_2x_coco.py', det_score_thr=0.9, device='cuda:0', label_map='../tools/data/skeleton/label_map_ntu120.txt', out_filename='../demo/demo_ske.mp4', pose_checkpoint='https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth', pose_config='../demo/hrnet_w32_coco_256x192.py', short_side=480, video='../demo/demo.mp4')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.argv = ['../demo/demo_skeleton.py', '../demo/demo.mp4', '../demo/demo_ske.mp4']\n",
    "# 暂时先用demo_ske的配置，后面改为自己的\n",
    "args = parse_args()\n",
    "# args.checkpoint = ''\n",
    "args.config = '../configs/skeleton/posec3d/slowonly_r50_u48_240e_ntu120_xsub_keypoint.py'\n",
    "args.det_config = '../demo/faster_rcnn_r50_fpn_2x_coco.py'\n",
    "args.pose_config='../demo/hrnet_w32_coco_256x192.py'\n",
    "args.label_map='../tools/data/skeleton/label_map_ntu120.txt'\n",
    "args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_folder(folder_path, start_frame_index, end_frame_index):\n",
    "    '''\n",
    "    传入的路径是否存在，是否是文件夹，是否包含帧图\n",
    "    '''\n",
    "    if not osp.exists(folder_path) or not osp.isdir(folder_path):\n",
    "        return False\n",
    "    files = os.listdir(folder_path)\n",
    "    if 'img_00000.jpg' not in files:\n",
    "        print('img_00000.jpg 不存在')\n",
    "        return False\n",
    "    index_str = \"%05d\" % start_frame_index\n",
    "    if f'img_{index_str}.jpg' not in files:\n",
    "        print(f'img_{index_str}.jpg 不存在')\n",
    "        return False\n",
    "    index_str = \"%05d\" % end_frame_index\n",
    "    if f'img_{index_str}.jpg' not in files:\n",
    "        print(f'img_{index_str}.jpg 不存在')\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def check_frame_index(start_frame_index, end_frame_index):\n",
    "    '''\n",
    "    检查index参数是否合法\n",
    "    '''\n",
    "    if start_frame_index < 0 or end_frame_index < 0 or start_frame_index >= end_frame_index:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(check_folder('/home/fate/openmmlab/mmaction2/data/13621115_5_0', 5873, 7539))\n",
    "# print(check_frame_index(5873, 7539))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def frame_extraction(folder_path, start, end):\n",
    "    '''\n",
    "    读取帧文件夹中从start-end的帧图，返回帧图的路径列表和帧数组\n",
    "    '''\n",
    "    frame_paths = []\n",
    "    frames = []\n",
    "    new_h, new_w = None, None\n",
    "    for i in range(start, end):\n",
    "        index = \"%05d\" % i\n",
    "        filename = f'img_{index}.jpg'\n",
    "        frame_path = osp.join(folder_path, filename)\n",
    "        frame = cv2.imread(frame_path)\n",
    "\n",
    "        if new_h is None:\n",
    "            h, w, _ = frame.shape\n",
    "            new_w, new_h = mmcv.rescale_size((w, h), (480, np.Inf))\n",
    "\n",
    "        frame = mmcv.imresize(frame, (new_w, new_h))\n",
    "\n",
    "        frames.append(frame)\n",
    "        frame_paths.append(osp.join(folder_path, filename))\n",
    "\n",
    "    return frame_paths, frames\n",
    "\n",
    "# frame_paths, frames = frame_extraction('/home/fate/openmmlab/mmaction2/data/13621115_5_0', 5873, 7539)\n",
    "# len(frame_paths), len(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_skeleton_by_frames_folder_with_start_and_end(folder_path, start, end, pkl_dir):\n",
    "    '''\n",
    "    生成骨架pkl文件，保存到pkl_dir中\n",
    "    返回保存的文件名，文件名命名格式为：{start}.pkl命名\n",
    "    '''\n",
    "    if not osp.exists(pkl_dir):\n",
    "        os.mkdir(pkl_dir)\n",
    "    frame_paths, original_frames = frame_extraction(folder_path, start, end)\n",
    "    num_frame = len(frame_paths)\n",
    "    h, w, _ = original_frames[0].shape\n",
    "\n",
    "    # Get clip_len, frame_interval and calculate center index of each clip\n",
    "    config = mmcv.Config.fromfile(args.config)\n",
    "    config.merge_from_dict(args.cfg_options)\n",
    "    for component in config.data.test.pipeline:\n",
    "        if component['type'] == 'PoseNormalize':\n",
    "            component['mean'] = (w // 2, h // 2, .5)\n",
    "            component['max_value'] = (w, h, 1.)\n",
    "\n",
    "    # Get Human detection results\n",
    "    det_results = detection_inference(args, frame_paths)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    pose_results = pose_inference(args, frame_paths, det_results)\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    fake_anno = dict(\n",
    "        frame_dir='',\n",
    "        label=-1,\n",
    "        img_shape=(h, w),\n",
    "        original_shape=(h, w),\n",
    "        start_index=0,\n",
    "        modality='Pose',\n",
    "        total_frames=num_frame)\n",
    "    num_person = max([len(x) for x in pose_results])\n",
    "\n",
    "    num_keypoint = 17\n",
    "    keypoint = np.zeros((num_person, num_frame, num_keypoint, 2),\n",
    "                        dtype=np.float16)\n",
    "    keypoint_score = np.zeros((num_person, num_frame, num_keypoint),\n",
    "                              dtype=np.float16)\n",
    "    for i, poses in enumerate(pose_results):\n",
    "        for j, pose in enumerate(poses):\n",
    "            pose = pose['keypoints']\n",
    "            keypoint[j, i] = pose[:, :2]\n",
    "            keypoint_score[j, i] = pose[:, 2]\n",
    "    fake_anno['keypoint'] = keypoint\n",
    "    fake_anno['keypoint_score'] = keypoint_score\n",
    "\n",
    "    out_anno_filename = osp.join(pkl_dir, f'{start}.pkl')\n",
    "    print(f'\\nsaving anno file: {out_anno_filename}')\n",
    "\n",
    "    with open(out_anno_filename, 'wb') as f:\n",
    "        pickle.dump(fake_anno, f)\n",
    "    return out_anno_filename\n",
    "\n",
    "# make_out_anno_filename = make_skeleton_by_frames_folder_with_start_and_end('/home/fate/openmmlab/data/13621115_5_0', 5873, 5874, '/home/fate/openmmlab/data/pkls')\n",
    "\n",
    "# make_out_anno_filename"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_the_number_by_pred_labels_list(y_preds:list, label:int):\n",
    "    '''\n",
    "    给出模型预测的动作label序列列表，和需要计算的统计的动作label，计算出现的次数\n",
    "    计算次数的规则：连续出现的相同标签算做一次，各动作累计，\n",
    "    例如动作标签列表如[0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2]，将连续的相同标签合并之后如\n",
    "    [0,1,0,2,0,1,0,1,0,2]\n",
    "    则label= 0时，返回5，label=1时返回3，label=2时返回2\n",
    "    '''\n",
    "    if len(y_preds) == 0:\n",
    "        return -1\n",
    "    last = y_preds[0]\n",
    "    # seq = [y_preds[0]]\n",
    "    count_map = {y_preds[0] : 1}\n",
    "    for y_pred in y_preds:\n",
    "        if last != y_pred:\n",
    "            if y_pred in count_map:\n",
    "                count_map[y_pred] += 1\n",
    "            else:\n",
    "                count_map[y_pred] = 1\n",
    "        last = y_pred\n",
    "            \n",
    "    # print(count_map)\n",
    "    return count_map[label]\n",
    "\n",
    "# l = [0, 1, 1, 0, 0, 2, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 2]\n",
    "# count_the_number_by_pred_labels_list(l, 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_the_number_of_times_a_certain_type_of_action_appears_in_the_frames_folder(\n",
    "    folder_path: str,\n",
    "    start_frame_index: int,\n",
    "    end_frame_index: int,\n",
    "    window_size: int,\n",
    "    step: int,\n",
    "    action_label: int\n",
    "):\n",
    "    '''\n",
    "    folder_path: path of a folder which contains frames of a video,\n",
    "    start_frame_index,\n",
    "    end_frame_index,\n",
    "    window_size: size of slide window\n",
    "    step: slide step, 小于等于0，则step = window_size\n",
    "    action_label: 4 actions type :['get','put down','screw', 'pump']\n",
    "    '''\n",
    "    # 0. check params\n",
    "    if not check_folder(folder_path, start_frame_index, end_frame_index) or not check_frame_index(start_frame_index, end_frame_index):\n",
    "        print(\"请检查参数\")\n",
    "        return\n",
    "    if step <= 0:\n",
    "        step = window_size\n",
    "    start = start_frame_index\n",
    "    y_preds = []\n",
    "    print(f'start: {start_frame_index}, end_frame_index: {end_frame_index}; window_size: {window_size}; step: {step}')\n",
    "    while start <= end_frame_index:\n",
    "        end = end_frame_index if start + window_size > end_frame_index else start + window_size\n",
    "        # print(f'start:{start}; end:{end}')\n",
    "        pkl_file_fullpath = make_skeleton_by_frames_folder_with_start_and_end(folder_path, start, end, PKL_DIR)\n",
    "        model = init_recognizer(CONFIG_FILE, CHECKPOINT_FILE, device='cuda:0')\n",
    "        with open(pkl_file_fullpath, 'rb') as f:\n",
    "            anno = pickle.load(f)\n",
    "            results = inference_recognizer(model, anno)\n",
    "            y_pred = results[0][0]\n",
    "            y_preds.append(y_pred)\n",
    "        start += step\n",
    "    # print(y_preds)\n",
    "    return count_the_number_by_pred_labels_list(y_preds, action_label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start: 5873, end_frame_index: 7539; window_size: 90; step: 90\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 15.9 task/s, elapsed: 6s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 15.8 task/s, elapsed: 6s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/5873.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 19.8 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 21.0 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/5963.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 19.8 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 22.1 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6053.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 19.7 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 22.8 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6143.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 19.7 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 24.8 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6233.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 19.8 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 23.0 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6323.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 19.8 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 18.8 task/s, elapsed: 5s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6413.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 20.7 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 21.3 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6503.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 20.2 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 24.1 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6593.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 20.1 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 18.4 task/s, elapsed: 5s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6683.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 20.4 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 21.0 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6773.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 21.1 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 21.1 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6863.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 21.1 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 21.3 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/6953.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 20.9 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 25.0 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/7043.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 20.1 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 21.4 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/7133.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 20.4 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 23.3 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/7223.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 20.6 task/s, elapsed: 4s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 22.8 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/7313.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 19.8 task/s, elapsed: 5s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 90/90, 20.5 task/s, elapsed: 4s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/7403.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n",
      "load checkpoint from http path: http://download.openmmlab.com/mmdetection/v2.0/faster_rcnn/faster_rcnn_r50_fpn_2x_coco/faster_rcnn_r50_fpn_2x_coco_bbox_mAP-0.384_20200504_210434-a5d8aa15.pth\n",
      "Performing Human Detection for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 46/46, 19.9 task/s, elapsed: 2s, ETA:     0sload checkpoint from http path: https://download.openmmlab.com/mmpose/top_down/hrnet/hrnet_w32_coco_256x192-c78dce93_20200708.pth\n",
      "Performing Human Pose Estimation for each frame\n",
      "[>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>] 46/46, 22.5 task/s, elapsed: 2s, ETA:     0s\n",
      "saving anno file: ./pkl_dir/7493.pkl\n",
      "load checkpoint from local path: ../checkpoints/ntu60_keypoints_4labels.pth\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_the_number_of_times_a_certain_type_of_action_appears_in_the_frames_folder(\n",
    "    '/home/fate/openmmlab/mmaction2/data/13621115_5_0',\n",
    "    5873,\n",
    "    7539,\n",
    "    30 * 3,\n",
    "    0,\n",
    "    1\n",
    ") ## 3min19.2s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be40d7e6a4cd261be2e5a1fb6fd78b6abc421e8295ca616a082915926260a7a1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
